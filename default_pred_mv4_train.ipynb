{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac6398be",
   "metadata": {},
   "source": [
    "# Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eb79ed",
   "metadata": {},
   "source": [
    "Criar um modelo de machine learning para fazer a previsão de inadimplencia de clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa540d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulação de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import redshift_connector\n",
    "\n",
    "# Converter dados para date\n",
    "from datetime import datetime\n",
    "\n",
    "# Modelos de Machine Learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import classification_report, log_loss\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Vizualização de dados\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Balanceamento das classes\n",
    "from imblearn.over_sampling import SMOTE\n",
    "#from imblearn.under_sampling import NearMiss\n",
    "#import imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a311b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parando as mensagens de warnings\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5a6ac",
   "metadata": {},
   "source": [
    "# Sumario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3c5167",
   "metadata": {},
   "source": [
    "1 - Carregando o Dataset\n",
    "2 - SQL utilizada para retirar os dados do BD IBM\n",
    "3 - Convertendo os tipos dos dados\n",
    "4 - Análise exploratória\n",
    "5 - Correlação dos Dados\n",
    "6 - Transformações nos dados e balanceamento de classes\n",
    "7 - Testando vários modelos de classificação\n",
    "8 - Procedimentos para entregar novos dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9171d2",
   "metadata": {},
   "source": [
    "## 1 - Carregando o Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c0045",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Carregando o arquivo CSV dos dados que foram exportados do redshift\n",
    "\n",
    "dataset = pd.read_csv(\"d_clients_mv4.csv\")\n",
    "dataset_BTYD = pd.read_csv(\"BTYD_probs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc88c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passando o arquivo CSV para Dataframe \n",
    "\n",
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578b4bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784c80cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['risco_credito'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0f32bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['churn_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e228c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando os tipos dos dados\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9013a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03b4427",
   "metadata": {},
   "source": [
    "## 3 - Convertendo os tipos dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6951e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipo object -  Variáveis convertidas para objects\n",
    "cols_objects = ['cd_cliente','ds_tp_pessoa','risco_credito'\n",
    "               ,'churn_class']\n",
    "df[cols_objects] = df[cols_objects].astype(object)\n",
    "\n",
    "# Tipo Inteiro -  Convertendo as variáveis do tipo inteiro\n",
    "cols_ints = ['pedidos','recencia','churn_class_number','soma_qt_cancelada','soma_qt_pendente']\n",
    "df[cols_ints] = df[cols_ints].astype(int)\n",
    "\n",
    "# Tipo float -  Convertendo as variáveis do tipo float\n",
    "cols_float = ['saldo_credev','saldo_adiantamento','score_empresas','negativacoes','soma_vlr_cancelado','soma_vlr_pendente']\n",
    "df[cols_float] = df[cols_float].astype(float)\n",
    "\n",
    "# Tipo Date -  Convertendo variáveis datetime - não vamos usar dados temporais neste modelo\n",
    "df['ano_mes_dt_cad_rep'] = df['ano_mes_dt_cad_rep'] = pd.to_datetime(df['ano_mes_dt_cad_rep'], format='%Y-%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28749e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma lista das variáveis numéricas para posterior análise\n",
    "\n",
    "cols_numerics = ['pedidos','recencia','churn_class_number','soma_qt_cancelada','soma_qt_pendente',\n",
    "                'saldo_credev','saldo_adiantamento','score_empresas','negativacoes','soma_vlr_cancelado','soma_vlr_pendente']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f21df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando novamente os tipos dos dados\n",
    "# Agora todos as colunas estão com os tipos de dados corretos\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a570fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorização dummy\n",
    "dummies = ['risco_credito', 'volume_compra', 'ds_tp_pessoa']\n",
    "\n",
    "\n",
    "num_df = pd.get_dummies(df[dummies])\n",
    "num_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61abfcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,num_df],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393bf5ab",
   "metadata": {},
   "source": [
    "## 4 - Análise exploratória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f958f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando a proporção de classes churn\n",
    "# Vai ser necessário equilivrar as classes para treinar o modelo\n",
    "# A variável está codificada 0 para ativo recorrente 1 para em churn\n",
    "\n",
    "df['churn_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb33f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando o formato do dataset\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c3ba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfac612a",
   "metadata": {},
   "source": [
    "## 5 - Correlação dos Dados\n",
    "\n",
    "Analisamos a correlação entre as variáveis numéricas e entre as variáveis numéricas e a variável alvo (o que estamos querendo analisar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe6af23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Vai ser necessário filtrar as colunas de valor pois elas apresentam dados \n",
    "# colineares(colunas se tratando do mesmo tipo de dado)\n",
    "\n",
    "df.corr(\"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc7ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c272826",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,dataset_BTYD],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d8818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisando as correlações com a TX churn existem algumas variáveis com baixa correlação\n",
    "# Vou remover essas variáveis para deixar o modelo mais generalista\n",
    "# Vou aproveitar e remover as variáveis que não vão entrar no modelo\n",
    "# Pois o modelo pode ficar enviezado por essas variáveis fazerem parte da regra de negócio que classifica o churn\n",
    "vars_baixa_corr = ['cd_cliente','ds_tp_pessoa','volume_compra','risco_credito','churn_class','recencia','ano_mes_dt_cad_rep','saldo_credev'\n",
    "                  ,'frequency','recency','T','monetary_value','pred_num_trans_30','pred_trans_avg_value','CLV']\n",
    "df_2 = df.drop(columns=vars_baixa_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887c4be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando as colunas do novo dataset com as variáveis com uma correlação relativamente aceitavel\n",
    "df_2 = df.dropna()\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daee5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passando a variável target para o tipo fator novamente\n",
    "\n",
    "df_2['churn_class_number'] = df_2['churn_class_number'].astype(object)\n",
    "df_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958572d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando a distribuição das classes 0 e 1\n",
    "df_2['churn_class_number'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc43037",
   "metadata": {},
   "source": [
    "## 6 - Transformações nos dados e balanceamento de classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114b1f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalização das variáveis numéricas, vou colocar elas todas na mesma escala\n",
    "# Antes disso vou criar meu dataset de preditores e dataset de target   \n",
    "\n",
    "preditores = ['saldo_adiantamento','score_empresas','negativacoes','pedidos','risco_credito_Risco Baixo',\n",
    "             'risco_credito_Risco Considerável','risco_credito_Risco Iminente','risco_credito_Risco Muito Baixo',\n",
    "             'risco_credito_Risco Médio','risco_credito_Risco Mínimo','risco_credito_Risco Relevante','volume_compra_ACIMA DE 90k',\n",
    "             'volume_compra_ATÉ 10k','volume_compra_DE 10k A 30k','volume_compra_DE 30k A 60k'\n",
    "              ,'ds_tp_pessoa_Fisica','ds_tp_pessoa_Juridica','soma_qt_cancelada','soma_qt_pendente','prob_alive']\n",
    "target = ['churn_class_number']\n",
    "\n",
    "X_ = df_2[preditores]\n",
    "Y_ = df_2[target].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bda881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanceamento de classes com SMOTE\n",
    "smt = SMOTE()\n",
    "\n",
    "X, Y = smt.fit_resample(X_, Y_)\n",
    "Y = Y.astype(int)\n",
    "# Nova distribuição das classes balanceadas\n",
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalização dos dados\n",
    "\n",
    "#min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#X = min_max_scaler.fit_transform(X)\n",
    "#X_ = min_max_scaler.fit_transform(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora vou fazer a divisão dos dados de treino e teste com proporção de 30% para teste\n",
    "# Divisão em dados de treino e de teste\n",
    "# Não irei balancear as classes para o modelo RF pois apresenta um desempenho mt bom sem o balanceamento\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.3)\n",
    "x_train_, x_valid_, y_train_, y_valid_ = train_test_split(X_, Y_, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7fe76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimindo o número de exemplos (observações) em cada dataset\n",
    "print(\"Exemplos de Treino: {}\".format(len(x_train)))\n",
    "print(\"Exemplos de Validação: {}\".format(len(y_valid)))\n",
    "print(\"Exemplos de Teste: {}\".format(len(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3988b87f",
   "metadata": {},
   "source": [
    "## 7 - Testando vários modelos de classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f354da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando vários modelos para teste\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# modelLR = LogisticRegression(penalty='none')\n",
    "# modelLR.fit(x_train, y_train)\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "modelRF = RandomForestClassifier(max_depth=8, max_features=10, min_samples_leaf=4,\n",
    "                       min_samples_split=10)\n",
    "modelRF.fit(x_train_, y_train_)\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "modelGB = GradientBoostingClassifier(loss='exponential', n_estimators=150)\n",
    "modelGB.fit(x_train, y_train)\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/svm.html\n",
    "# modelSVM = svm.SVC()\n",
    "# modelSVM.fit(x_train, y_train)\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "# modelKN = KNeighborsClassifier(n_neighbors=3)\n",
    "# modelKN.fit(x_train, y_train)   \n",
    "    \n",
    "# https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "# modelNB = GaussianNB()\n",
    "# modelNB = modelNB.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc8bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Os modelos RF e GB demonstraram melhor desemprenho dos demais, \n",
    "# por isso irei aplicar o RSearch no GB e RF para otimizar seus parametros\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "\n",
    "# RF\n",
    "# Definição dos parâmetros\n",
    "#param_dist = {\"max_depth\": [1, 3, 7, 8, 12, None],\n",
    "#              \"max_features\": [8, 9, 10, 11, 16, 22],\n",
    "#              \"min_samples_split\": [8, 10, 11, 14, 16, 19],\n",
    "#              \"min_samples_leaf\": [1, 2, 3, 4, 5, 6, 7],\n",
    "#              \"bootstrap\": [True, False]}\n",
    "\n",
    "# Para o classificador criado com ExtraTrees, testamos diferentes combinações de parâmetros\n",
    "# rsearch = RandomizedSearchCV(modelRF, param_distributions = param_dist, n_iter = 35, return_train_score = True)  \n",
    "\n",
    "# Aplicando o resultado ao conjunto de dados de treino e obtendo o score\n",
    "# rsearch.fit(x_train_, y_train_)\n",
    "\n",
    "# Resultados \n",
    "# rsearch.cv_results_\n",
    "\n",
    "# Imprimindo o melhor estimador\n",
    "# modelRF = rsearch.best_estimator_\n",
    "# print (modelRF)\n",
    "\n",
    "# Aplicando o melhor estimador para realizar as previsões\n",
    "# y_pred = modelRF.predict(x_valid_)\n",
    "\n",
    "# Confusion Matrix\n",
    "# confusionMatrix = confusion_matrix(y_valid_, y_pred)\n",
    "# print(confusionMatrix)\n",
    "\n",
    "# Acurácia\n",
    "# accuracy = accuracy_score(y_valid_, y_pred)\n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Os modelos RF e GB demonstraram melhor desemprenho dos demais, \n",
    "# por isso irei aplicar o RSearch no GB e RF para otimizar seus parametros\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "\n",
    "# GB\n",
    "# Definição dos parâmetros\n",
    "# param_dist = {\"loss\": ['deviance', 'exponential'],\n",
    "#               \"learning_rate\": [0.1, 0.03, 0.5, 0.7],\n",
    "#               \"n_estimators\": [50, 100, 150, 200],\n",
    "#               \"subsample\": [1.0,3.0],\n",
    "#               \"criterion\": ['friedman_mse', 'mse']}\n",
    "\n",
    "# Para o classificador criado com ExtraTrees, testamos diferentes combinações de parâmetros\n",
    "# rsearch = RandomizedSearchCV(modelGB, param_distributions = param_dist, n_iter = 35, return_train_score = True)  \n",
    "\n",
    "# Aplicando o resultado ao conjunto de dados de treino e obtendo o score\n",
    "# rsearch.fit(x_train_, y_train_)\n",
    "\n",
    "# Resultados \n",
    "# rsearch.cv_results_\n",
    "\n",
    "# Imprimindo o melhor estimador\n",
    "# modelGB = rsearch.best_estimator_\n",
    "# print (modelGB)\n",
    "\n",
    "# Aplicando o melhor estimador para realizar as previsões\n",
    "# y_pred = modelGB.predict(x_valid_)\n",
    "\n",
    "# Confusion Matrix\n",
    "# confusionMatrix = confusion_matrix(y_valid_, y_pred)\n",
    "# print(confusionMatrix)\n",
    "\n",
    "# Acurácia\n",
    "# accuracy = accuracy_score(y_valid_, y_pred)\n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d92b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictLR = modelLR.predict(x_valid)\n",
    "# print('Logistic Regression: \\n', classification_report(predictLR, y_valid))\n",
    "# print('Logistic Regression Accuracy: ', accuracy_score(predictLR, y_valid))\n",
    "\n",
    "predictRF = modelRF.predict(x_valid_)\n",
    "print('Random Forest Classifier : \\n', classification_report(predictRF, y_valid_))\n",
    "print('Random Forest Classifier Accuracy: ', accuracy_score(predictRF, y_valid_))\n",
    "\n",
    "predictGB = modelGB.predict(x_valid)\n",
    "print('Gradient Boost Classifier : \\n', classification_report(predictGB, y_valid))\n",
    "print('Gradient Boost Classifier Accuracy: ', accuracy_score(predictGB, y_valid))\n",
    "\n",
    "# predictSVM = modelSVM.predict(x_valid)\n",
    "# print('SVM : \\n', classification_report(predictSVM, y_valid))\n",
    "# print('SVM Accuracy: ', accuracy_score(predictSVM, y_valid))\n",
    "\n",
    "# predictKN = modelKN.predict(x_valid)\n",
    "# print('KN : \\n', classification_report(predictKN, y_valid))\n",
    "# print('KN Accuracy: ', accuracy_score(predictKN, y_valid))\n",
    "\n",
    "# predictNB = modelNB.predict(x_valid)\n",
    "# print('NB : \\n', classification_report(predictNB, y_valid))\n",
    "# print('NB Accuracy: ', accuracy_score(predictNB, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb9986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix do Modelo Final\n",
    "# print (\"Confusion matrix\")\n",
    "# print('LR',confusion_matrix(predictLR, y_valid))\n",
    "print('RF',confusion_matrix(predictRF, y_valid_))\n",
    "print('GB',confusion_matrix(predictGB, y_valid))\n",
    "# print('SVM',confusion_matrix(predictSVM, y_valid))\n",
    "# print('KN',confusion_matrix(predictKN, y_valid))\n",
    "# print('NB',confusion_matrix(predictNB, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd3a49b",
   "metadata": {},
   "source": [
    "## 8 - Procedimentos para entregar novos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o arquivo CSV com novos dados para previsão\n",
    "\n",
    "novos_dados = pd.read_csv(\"novosdados_mv4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329bb86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "novos_dados.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d80f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passando o arquivo CSV para Dataframe \n",
    "df_novos_dados = pd.DataFrame(novos_dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c7a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipo object -  Variáveis convertidas para objects\n",
    "cols_objects = ['cd_cliente','ds_tp_pessoa','risco_credito'\n",
    "               ,'churn_class']\n",
    "df_novos_dados[cols_objects] = df_novos_dados[cols_objects].astype(object)\n",
    "\n",
    "# Tipo Inteiro -  Convertendo as variáveis do tipo inteiro\n",
    "cols_ints = ['pedidos','recencia','churn_class_number']\n",
    "df_novos_dados[cols_ints] = df_novos_dados[cols_ints].astype(int)\n",
    "\n",
    "# Tipo float -  Convertendo as variáveis do tipo float\n",
    "cols_float = ['saldo_credev','saldo_adiantamento','score_empresas','negativacoes']\n",
    "df_novos_dados[cols_float] = df_novos_dados[cols_float].astype(float)\n",
    "\n",
    "# Tipo Date -  Convertendo variáveis datetime - não vamos usar dados temporais neste modelo\n",
    "df_novos_dados['ano_mes_dt_cad_rep'] = df_novos_dados['ano_mes_dt_cad_rep'] = pd.to_datetime(df_novos_dados['ano_mes_dt_cad_rep'], format='%Y-%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80054816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorização dummy\n",
    "dummies = ['risco_credito', 'volume_compra', 'ds_tp_pessoa']\n",
    "\n",
    "\n",
    "num_df_new = pd.get_dummies(df_novos_dados[dummies])\n",
    "num_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374fd3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = pd.concat([df_novos_dados,num_df_new],axis=1)\n",
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f3a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = pd.concat([df_3,dataset_BTYD],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b1750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropando valores NA\n",
    "df_3 = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b651608",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3['churn_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5ae1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf782cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colocando os dados no mesmo padrão dos dados de treino\n",
    "\n",
    "\n",
    "preditores_new = ['saldo_adiantamento','score_empresas','negativacoes','pedidos','risco_credito_Risco Baixo',\n",
    "             'risco_credito_Risco Considerável','risco_credito_Risco Iminente','risco_credito_Risco Muito Baixo',\n",
    "             'risco_credito_Risco Médio','risco_credito_Risco Mínimo','risco_credito_Risco Relevante','volume_compra_ACIMA DE 90k',\n",
    "             'volume_compra_ATÉ 10k','volume_compra_DE 10k A 30k','volume_compra_DE 30k A 60k'\n",
    "              ,'ds_tp_pessoa_Fisica','ds_tp_pessoa_Juridica','soma_qt_cancelada','soma_qt_pendente','prob_alive']\n",
    "X_new_2 = df_3[preditores_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9070b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalização dos dados\n",
    "#min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#X_new = min_max_scaler.fit_transform(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c92fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo previsão com novos dados\n",
    "# predictLR_2 = modelLR.predict(X_new)\n",
    "# predictLR_2_prob = modelLR.predict_proba(X_new)\n",
    "# print('Logistic Regression: \\n', classification_report(predictLR_2, df_2['churn_class_number']))\n",
    "# print('Logistic Regression Accuracy: ', accuracy_score(predictLR_2, df_2['churn_class_number']))\n",
    "\n",
    "predictRF_2 = modelRF.predict(X_new_2)\n",
    "predictRF_2_prob = modelRF.predict_proba(X_new_2)\n",
    "print('Random Forest Classifier : \\n', classification_report(predictRF_2, df_3['churn_class_number']))\n",
    "print('Random Forest Classifier Accuracy: ', accuracy_score(predictRF_2, df_3['churn_class_number']))\n",
    "\n",
    "predictGB_2 = modelGB.predict(X_new_2)\n",
    "predictGB_2_prob = modelGB.predict_proba(X_new_2)\n",
    "print('Gradient Boost Classifier : \\n', classification_report(predictGB_2, df_3['churn_class_number']))\n",
    "print('Gradient Boost Classifier Accuracy: ', accuracy_score(predictGB_2, df_3['churn_class_number']))\n",
    "\n",
    "# predictSVM_2 = modelSVM.predict(X_new)\n",
    "# print('SVM Classifier : \\n', classification_report(predictSVM_2, df_2['churn_class_number']))\n",
    "# print('SVM Classifier Accuracy: ', accuracy_score(predictSVM_2, df_2['churn_class_number']))\n",
    "\n",
    "# predictKN_2 = modelKN.predict(X_new)\n",
    "# predictKN_2_prob = modelKN.predict_proba(X_new)\n",
    "# print('KN Classifier : \\n', classification_report(predictKN_2, df_2['churn_class_number']))\n",
    "# print('KN Classifier Accuracy: ', accuracy_score(predictKN_2, df_2['churn_class_number']))\n",
    "\n",
    "# predictNB_2 = modelNB.predict(X_new)\n",
    "# predictNB_2_prob = modelNB.predict_proba(X_new)\n",
    "# print('NB Classifier : \\n', classification_report(predictNB_2, df_2['churn_class_number']))\n",
    "# print('NB Classifier Accuracy: ', accuracy_score(predictNB_2, df_2['churn_class_number']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6738638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando as colunas de previsão de cada modelo no dataset\n",
    "# df_2['predictLR_2'] = list(predictLR_2)\n",
    "df_3['predictRF_2'] = list(predictRF_2)\n",
    "df_3['predictGB_2'] = list(predictGB_2)\n",
    "# df_2['predictSVM_2'] = list(predictSVM_2)\n",
    "# df_2['predictKN_2'] = list(predictKN_2)\n",
    "# df_2['predictNB_2'] = list(predictNB_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd0fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando as colunas de previsão probabilistica de cada modelo no dataset\n",
    "# df_2['predictLR_2_prob'] = list(predictLR_2_prob)\n",
    "df_2['predictRF_2_prob'] = list(predictRF_2_prob)\n",
    "df_2['predictGB_2_prob'] = list(predictGB_2_prob)\n",
    "# df_2['predictKN_2_prob'] = list(predictKN_2_prob)\n",
    "# df_2['predictNB_2_prob'] = list(predictNB_2_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c07e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.to_csv('predictions_new_mv4.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07de8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e72fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3['churn_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_2['predictLR_2'].value_counts())\n",
    "print(df_3['predictRF_2'].value_counts())\n",
    "print(df_3['predictGB_2'].value_counts())\n",
    "# print(df_2['predictSVM_2'].value_counts())\n",
    "# print(df_2['predictKN_2'].value_counts())\n",
    "# print(df_2['predictNB_2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix do Modelo Final\n",
    "# print (\"Confusion matrix LR\")\n",
    "# print(confusion_matrix(df_2['churn_class_number'], predictLR_2))\n",
    "print (\"Confusion matrix RF\")\n",
    "print(confusion_matrix(df_3['churn_class_number'], predictRF_2))\n",
    "print (\"Confusion matrix GB\")\n",
    "print(confusion_matrix(df_3['churn_class_number'], predictGB_2))\n",
    "# print (\"Confusion matrix SVM\")\n",
    "# print(confusion_matrix(df_2['churn_class_number'], predictSVM_2))\n",
    "# print (\"Confusion matrix KN\")\n",
    "# print(confusion_matrix(df_2['churn_class_number'], predictKN_2))\n",
    "# print (\"Confusion matrix NB\")\n",
    "# print(confusion_matrix(df_2['churn_class_number'], predictNB_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda19f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot da confusion matrix\n",
    "# ax = plt.subplot()\n",
    "# sns.heatmap(confusion_matrix(df_2['churn_class_number'], predictLR_2), annot=True, fmt='g', ax=ax);\n",
    "\n",
    "# labels, title and ticks\n",
    "# ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "# ax.set_title('Confusion Matrix LR'); \n",
    "# ax.xaxis.set_ticklabels(['0', '1']); ax.yaxis.set_ticklabels(['0', '1']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7653b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot da confusion matrix\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(confusion_matrix(df_3['churn_class_number'], predictRF_2), annot=True, fmt='g', ax=ax);\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix RF'); \n",
    "ax.xaxis.set_ticklabels(['0', '1']); ax.yaxis.set_ticklabels(['0', '1']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c95c22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot da confusion matrix\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(confusion_matrix(df_3['churn_class_number'], predictGB_2), annot=True, fmt='g', ax=ax);\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix GB'); \n",
    "ax.xaxis.set_ticklabels(['0', '1']); ax.yaxis.set_ticklabels(['0', '1']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1303e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a646f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot da confusion matrix\n",
    "# ax = plt.subplot()\n",
    "# sns.heatmap(confusion_matrix(df_2['churn_class_number'], predictSVM_2), annot=True, fmt='g', ax=ax);\n",
    "\n",
    "# labels, title and ticks\n",
    "# ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "# ax.set_title('Confusion Matrix SVM'); \n",
    "# ax.xaxis.set_ticklabels(['0', '1']); ax.yaxis.set_ticklabels(['0', '1']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4fe43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot da confusion matrix\n",
    "# ax = plt.subplot()\n",
    "# sns.heatmap(confusion_matrix(df_2['churn_class_number'], predictKN_2), annot=True, fmt='g', ax=ax);\n",
    "\n",
    "# labels, title and ticks\n",
    "# ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "# ax.set_title('Confusion Matrix KN'); \n",
    "# ax.xaxis.set_ticklabels(['0', '1']); ax.yaxis.set_ticklabels(['0', '1']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd4b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot da confusion matrix\n",
    "# ax = plt.subplot()\n",
    "# sns.heatmap(confusion_matrix(df_2['churn_class_number'], predictNB_2), annot=True, fmt='g', ax=ax);\n",
    "\n",
    "# labels, title and ticks\n",
    "# ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "# ax.set_title('Confusion Matrix NB'); \n",
    "# ax.xaxis.set_ticklabels(['0', '1']); ax.yaxis.set_ticklabels(['0', '1']);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
